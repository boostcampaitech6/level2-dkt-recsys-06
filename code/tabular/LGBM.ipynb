{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "M_tLPVXsMpJZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import argparse\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "import xgboost as XG\n",
        "from lightgbm import LGBMClassifier\n",
        "import optuna\n",
        "import joblib\n",
        "import json\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "fe=\"Y\" # train시 valid set 쓸건지 안쓸건지 안 쓸꺼면 --fe N\n",
        "trials=1 #랜덤 조합으로 몇번\n",
        "    \n",
        "\n",
        "## EDA 바뀔시 ##\n",
        "file_name=\"FE_v7.csv\"\n",
        "\n",
        "cat_feats=['userID', 'assessmentItemID', 'testId', \n",
        "       'KnowledgeTag', \n",
        "       #'Itemseq', 'SolvingTime', 'CumulativeTime', \n",
        "       'Month',\n",
        "       'DayOfWeek', 'TimeOfDay', 'WeekOfYear', \n",
        "       #'UserAvgSolvingTime',\n",
        "       #'CumulativeItemCount', 'Item_last7days', 'Item_last30days',\n",
        "       #'PastItemCount', 'CumulativeUserItemAnswerRate', 'ItemAnswerRate',\n",
        "       #'AverageItemSolvingTime_Correct', 'AverageItemSolvingTime_Incorrect',\n",
        "       #'AverageItemSolvingTime', 'Difference_SolvingTime_AvgItemSolvingTime',\n",
        "       #'UserTagAvgSolvingTime', 'TagAnswerRate',\n",
        "       #'CumulativeUserTagAverageAnswerRate',\n",
        "       #'CumulativeUserTagExponentialAverage', 'UserCumulativeTagCount',\n",
        "       'UserRecentTagAnswer', 'PreviousItemAnswer', \n",
        "       #'TestAnswerRate',\n",
        "       #'categorize_solvingTime', \n",
        "       'categorize_ItemAnswerRate', \n",
        "       'categorize_TagAnswerRate', 'categorize_TestAnswerRate',\n",
        "       'categorize_CumulativeUserItemAnswerRate',\n",
        "       #'categorize_CumulativeUserTagAverageAnswerRate',\n",
        "       'categorize_CumulativeUserTagExponentialAverage'\n",
        "                                                \n",
        "    ]\n",
        "\n",
        "## 일반 ##\n",
        "n_fold =5\n",
        "seed = 42\n",
        "data_dir =\"../../data/\"\n",
        "model_dir =\"model/\"\n",
        "model_name =\"best_model.pt\"\n",
        "output_dir =\"submit/\"\n",
        "test_file_name =\"test_data.csv\"\n",
        "\n",
        "def set_seeds(seed: int = 42):\n",
        "    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, train: pd.DataFrame):\n",
        "        self.train = train\n",
        "\n",
        "    def restruct_data(self) -> dict:\n",
        "        # train과 test 분할\n",
        "        data = {}\n",
        "        df = self.train\n",
        "        train = df[df[\"answerCode\"] >= 0]\n",
        "        test = df[df[\"answerCode\"] == -1]\n",
        "        data[\"train\"], data[\"test\"] = train, test\n",
        "        return data\n",
        "    \n",
        "    \n",
        "    def split_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        data의 구성\n",
        "        data['train'] : 전체 user_id에 대한 데이터(Test에 있는 User에 대해서는 이미 마지막으로 푼 문제 정보가 없음)\n",
        "        data['train_split'] : 전체 user_id별 마지막으로 푼 문제를 제외한 데이터\n",
        "        data['valid'] : 전체 user_id별 마지막으로 푼 문제에 대한 데이터\n",
        "        \"\"\"\n",
        "        data = self.restruct_data()\n",
        "        FE_train= type_conversion(data[\"train\"])\n",
        "        \n",
        "        train = data['train'].copy()\n",
        "        train[\"is_valid\"] = [False] * train.shape[0]\n",
        "        idx_last = train.drop_duplicates(subset=\"userID\", keep=\"last\").index\n",
        "        train.loc[idx_last, \"is_valid\"] = True\n",
        "\n",
        "        train, valid = train[train[\"is_valid\"] == False], train[train[\"is_valid\"] == True]\n",
        "        data['train'] = train.drop(\"is_valid\", axis=1)\n",
        "        data['valid'] = valid.drop(\"is_valid\", axis=1)\n",
        "\n",
        "        print(f'{data[f\"train\"].shape[0]} train data')\n",
        "        print(f'{data[f\"valid\"].shape[0]} valid data')\n",
        "\n",
        "        return data, FE_train\n",
        "\n",
        "def type_conversion(df):\n",
        "        # [FEAT] integer여도 범주형으로 취급 가능\n",
        "        for feature in cat_feats:\n",
        "                df[feature] = df[feature].astype('category')\n",
        "\n",
        "        return df\n",
        "\n",
        "class Preprocess:\n",
        "    def __init__(self, data: dict):\n",
        "        self.data = data\n",
        "\n",
        "    def preprocess(self,cat_feats) -> dict:\n",
        "        self.data[\"train_x\"] = self.data[\"train\"].drop(\"answerCode\", axis=1)\n",
        "        self.data[\"train_y\"] = self.data[\"train\"][\"answerCode\"]\n",
        "\n",
        "        self.data[\"valid_x\"] = self.data[\"valid\"].drop(\"answerCode\", axis=1)\n",
        "        self.data[\"valid_y\"] = self.data[\"valid\"][\"answerCode\"]\n",
        "\n",
        "        self.data[\"test\"] = self.data[\"test\"].drop(\"answerCode\", axis=1)\n",
        "\n",
        "        # as category: integer여도 범주형으로 취급 가능\n",
        "        for state in [\"train_x\", \"valid_x\", \"test\"]:\n",
        "            df = self.data[state]\n",
        "                \n",
        "            for feature in cat_feats:\n",
        "                df[feature] = df[feature].astype('category')\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        return self.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#logger = get_logger(logger_conf=logging_conf)\n",
        "\n",
        "# optuna\n",
        "def objective(trial, FEATURE,data):\n",
        "    params_LGBM = {\n",
        "        'random_state':seed,\n",
        "        #'objective': 'binary', \n",
        "        'force_row_wise':True,\n",
        "        'metric': 'auc',  # 평가 지표로 AUC 사용\n",
        "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
        "        #'n_estimators' : trial.suggest_int('num_round', 1000, 5000),  \n",
        "        \n",
        "        #'num_leaves': trial.suggest_int('num_leaves', 10, 200),  # 트리의 최대 리프 노드 개수\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),  # 학습 속도\n",
        "        #'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),  # 각 트리에 사용할 특성의 비율\n",
        "        #'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),  # 리프 노드에 필요한 최소 데이터 수\n",
        "        #'max_depth': trial.suggest_int('max_depth', 3, 15),  # 트리의 최대 깊이\n",
        "        #'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),  # L1 정규화 강도\n",
        "        #'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),  # L2 정규화 강도\n",
        "        #'min_split_gain': trial.suggest_float('min_split_gain', 0.1, 1.0),  # 분할 최소 이득\n",
        "        #'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-8, 1e+2),  # 자식 노드에서 필요한 최소 가중치 합계\n",
        "        #'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),  # 각 트리에 사용할 데이터의 비율\n",
        "        #'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),  # 데이터 샘플링 빈도\n",
        "        #'max_bin': trial.suggest_int('max_bin', 32, 512),  # 히스토그램 분할 중 최대 bin 수\n",
        "        #'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 10.0),  # 양성 클래스의 가중치\n",
        "        #'cat_smooth': trial.suggest_float('cat_smooth', 1.0, 10.0),  # 카테고리 특징을 부드럽게 하는 파라미터\n",
        "        }\n",
        "    n_splits = n_fold\n",
        "    sgkf = StratifiedGroupKFold(n_splits=n_splits)\n",
        "    bst = LGBMClassifier(**params_LGBM)\n",
        "    score = []\n",
        "\n",
        "    for train_index, valid_index in sgkf.split(data['train_x'], data['train_y'], groups=data['train_x']['userID']):\n",
        "        bst.fit(X=data['train_x'].iloc[train_index][FEATURE], y=data['train_y'].iloc[train_index], \n",
        "                    eval_set=[(data['valid_x'][FEATURE], data['valid_y'])],\n",
        "                    )\n",
        "\n",
        "        #y_pred_proba = bst.predict_proba(data[\"valid_x\"][FEATURE])[:, 1]\n",
        "        y_pred_proba = bst.predict(data['valid_x'][FEATURE])\n",
        "        y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred_proba]\n",
        "\n",
        "        # Calculate accuracy and AUC\n",
        "        accuracy = accuracy_score(data['valid_y'], y_pred_binary)\n",
        "        auc = roc_auc_score(data['valid_y'], y_pred_proba)\n",
        "        print('Accuracy: {:.4f}'.format(accuracy))\n",
        "        print('AUC: {:.4f}'.format(auc))\n",
        "\n",
        "        # Append the AUC score to the list\n",
        "        score.append(auc)\n",
        "\n",
        "    # Calculate and print the average AUC score\n",
        "    result = sum(score) / len(score)\n",
        "    print('Average AUC: {:.4f}'.format(result))\n",
        "\n",
        "    return result  # auc 최대화하는 방향으로\n",
        "\n",
        "\n",
        "class boosting_model:\n",
        "    def __init__(self, FEATURE,data):\n",
        "        self.feature = FEATURE\n",
        "        self.data = data\n",
        "        \n",
        "        # Optuna 최적화\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(lambda trial: objective(trial,self.feature, self.data), n_trials=trials)\n",
        "\n",
        "        # 최적 하이퍼파라미터 출력\n",
        "        print('Hyperparameters: {}'.format(study.best_params))\n",
        "            \n",
        "        self.model = LGBMClassifier(\n",
        "               **study.best_params, objective = 'binary', metric = 'auc'\n",
        "            )\n",
        "            \n",
        "\n",
        "\n",
        "    def training(self, data, FEATURE,FE_train):\n",
        "        #logger.info(\"###start MODEL training ###\")\n",
        "        #logger.info(self.feature)\n",
        "\n",
        "\n",
        "        if fe == \"N\":\n",
        "            self.model.fit(\n",
        "                    FE_train[FEATURE],\n",
        "                    FE_train[\"answerCode\"],\n",
        "                    categorical_feature=cat_feats\n",
        "                    )\n",
        "        else:\n",
        "            print(\"Valid Data is used while training\")\n",
        "            n_splits = n_fold  \n",
        "            sgkf = StratifiedGroupKFold(n_splits=n_splits)\n",
        "\n",
        "            score = []\n",
        "            \n",
        "            for train_index, valid_index in sgkf.split(data[\"train_x\"], data[\"train_y\"], groups=data['train_x']['userID']):\n",
        "                self.model.fit(\n",
        "                        data[\"train_x\"].iloc[train_index][FEATURE],\n",
        "                        data[\"train_y\"].iloc[train_index],\n",
        "                        eval_set=(data[\"valid_x\"][FEATURE], data['valid_y']),\n",
        "                   \n",
        "                    )\n",
        "\n",
        "                # Prediction\n",
        "                y_pred_proba = self.model.predict(data[\"valid_x\"][FEATURE])\n",
        "                y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred_proba]\n",
        "\n",
        "                # Calculate accuracy and AUC\n",
        "                accuracy = accuracy_score(data.iloc[valid_index]['valid_y'], y_pred_binary)\n",
        "                auc = roc_auc_score(data.iloc[valid_index]['valid_y'], y_pred_proba)\n",
        "                print('Accuracy: {:.4f}'.format(accuracy))\n",
        "                print('AUC: {:.4f}'.format(auc))\n",
        "\n",
        "                # Append the AUC score to the list\n",
        "                score.append(auc)\n",
        "\n",
        "            # Calculate and print the average AUC score\n",
        "            result = sum(score) / len(score)\n",
        "            print('Average AUC: {:.4f}'.format(result))\n",
        "            model_type = 'lightgbm' \n",
        "     \n",
        "        \n",
        "        get_feature_importance(self.model, FEATURE, model_type)    \n",
        "\n",
        "\n",
        "    def inference(self, data,save_time,model_type):\n",
        "        # submission 제출하기 위한 코드\n",
        "        #test_pred = self.model.predict(data[\"test\"][self.feature])\n",
        "        test_pred = self.model.predict_proba(data[\"test\"][self.feature])[:, 1]\n",
        "\n",
        "        data[\"test\"][\"prediction\"] = test_pred\n",
        "        submission = data[\"test\"][\"prediction\"].reset_index(drop=True).reset_index()\n",
        "        submission.rename(columns={\"index\": \"id\"}, inplace=True)\n",
        "        submission_filename = f\"{model_type}_{save_time}.csv\"\n",
        "        submission.to_csv(\n",
        "            os.path.join(self.output_dir, submission_filename), index=False\n",
        "        )\n",
        "\n",
        "        # model save\n",
        "        joblib.dump(self.model, f'model/{model_type}_{save_time}.pkl')\n",
        "\n",
        "        # best parameter 저장\n",
        "        os.makedirs(f'log/{model_type}', exist_ok=True)\n",
        "        with open(f'log/{model_type}/{model_type}_{save_time}.json', 'w') as f:\n",
        "            json.dump(self.best_params, f)\n",
        "\n",
        "\n",
        "def get_feature_importance(model, feature_names, model_type):\n",
        "        importance = model.feature_importances_\n",
        "\n",
        "        feature_importance = np.array(importance)\n",
        "        feature_names = np.array(feature_names)\n",
        "    \n",
        "        # DataFrame 생성하고 정렬\n",
        "        fi_df = pd.DataFrame({'Feature Names': feature_names, 'Feature Importance': feature_importance})\n",
        "        fi_df = fi_df.sort_values(by='Feature Importance', ascending=False)\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"{model_type} Feature Importance:\")\n",
        "        print(fi_df)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### DATA LOAD ###\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2518514 train data\n",
            "7442 valid data\n",
            "### DATA PREPROCESSING ###\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-01-23 06:08:36,300] A new study created in memory with name: no-name-402cd90a-a9e7-4b45-8b51-d594fb15c127\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### HYPER PARAMETER TUNING - USING OPTUNA ###\n",
            "number of selected features: 29\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Number of positive: 1319966, number of negative: 694841\n",
            "[LightGBM] [Info] Total Bins 19076\n",
            "[LightGBM] [Info] Number of data points in the train set: 2014807, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655133 -> initscore=0.641678\n",
            "[LightGBM] [Info] Start training from score 0.641678\n",
            "Accuracy: 0.4863\n",
            "AUC: 0.5000\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Number of positive: 1319973, number of negative: 694840\n",
            "[LightGBM] [Info] Total Bins 19120\n",
            "[LightGBM] [Info] Number of data points in the train set: 2014813, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655134 -> initscore=0.641685\n",
            "[LightGBM] [Info] Start training from score 0.641685\n",
            "Accuracy: 0.4863\n",
            "AUC: 0.5000\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Number of positive: 1319983, number of negative: 694836\n",
            "[LightGBM] [Info] Total Bins 19087\n",
            "[LightGBM] [Info] Number of data points in the train set: 2014819, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655137 -> initscore=0.641698\n",
            "[LightGBM] [Info] Start training from score 0.641698\n",
            "Accuracy: 0.4863\n",
            "AUC: 0.5000\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Number of positive: 1319978, number of negative: 694828\n",
            "[LightGBM] [Info] Total Bins 19097\n",
            "[LightGBM] [Info] Number of data points in the train set: 2014806, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655139 -> initscore=0.641706\n",
            "[LightGBM] [Info] Start training from score 0.641706\n",
            "Accuracy: 0.4863\n",
            "AUC: 0.5000\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Number of positive: 1319976, number of negative: 694835\n",
            "[LightGBM] [Info] Total Bins 19094\n",
            "[LightGBM] [Info] Number of data points in the train set: 2014811, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.655136 -> initscore=0.641694\n",
            "[LightGBM] [Info] Start training from score 0.641694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-01-23 06:12:37,133] Trial 0 finished with value: 0.5 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.0013390699386409}. Best is trial 0 with value: 0.5.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.4863\n",
            "AUC: 0.5000\n",
            "Average AUC: 0.5000\n",
            "Hyperparameters: {'boosting_type': 'dart', 'learning_rate': 0.0013390699386409}\n",
            "### TRAIN ###\n",
            "Valid Data is used while training\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'iloc'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(model_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[20], line 76\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### TRAIN ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#logger.info(\"Start Training ...\")\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFEATURE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFE_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m######################## INFERENCE\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### INFERENCE ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[19], line 96\u001b[0m, in \u001b[0;36mboosting_model.training\u001b[0;34m(self, data, FEATURE, FE_train)\u001b[0m\n\u001b[1;32m     92\u001b[0m score \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, valid_index \u001b[38;5;129;01min\u001b[39;00m sgkf\u001b[38;5;241m.\u001b[39msplit(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_x\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_y\u001b[39m\u001b[38;5;124m\"\u001b[39m], groups\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_x\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserID\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m---> 96\u001b[0m             \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[train_index][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_x\u001b[39m\u001b[38;5;124m\"\u001b[39m][FEATURE],\n\u001b[1;32m     97\u001b[0m             data\u001b[38;5;241m.\u001b[39miloc[train_index][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_y\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     98\u001b[0m             eval_set\u001b[38;5;241m=\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_x\u001b[39m\u001b[38;5;124m\"\u001b[39m][FEATURE], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_y\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     99\u001b[0m        \n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_x\u001b[39m\u001b[38;5;124m\"\u001b[39m][FEATURE])\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'iloc'"
          ]
        }
      ],
      "source": [
        "# python main.py --model    # CAT, XG, LGBM   default=\"CAT\", \n",
        "\n",
        "# Boosting 계열, 수정할수 있는 파라미터\n",
        "# 1. FEATURE 선택\n",
        "# 2. train시 valid set 쓸건지 안쓸건지, default: Y\n",
        "# 3. optuna 시도 횟수, default: n_trials=10, 보통 100번이상이면 수렴됨\n",
        "# 4. optuna params\n",
        "\n",
        "#logger = get_logger(logger_conf=logging_conf)\n",
        "\n",
        "\n",
        "def main():\n",
        "    ######################## SELECT FEATURE\n",
        "    FEATURE = ['userID', 'assessmentItemID', 'testId', \n",
        "       'KnowledgeTag', 'Itemseq', 'SolvingTime', 'CumulativeTime', 'Month',\n",
        "       'DayOfWeek', 'TimeOfDay', 'WeekOfYear', 'UserAvgSolvingTime',\n",
        "       'CumulativeItemCount', 'Item_last7days', 'Item_last30days',\n",
        "       'PastItemCount', \n",
        "       #'CumulativeUserItemAnswerRate', 'ItemAnswerRate',\n",
        "       'AverageItemSolvingTime_Correct', 'AverageItemSolvingTime_Incorrect',\n",
        "       'AverageItemSolvingTime', 'Difference_SolvingTime_AvgItemSolvingTime',\n",
        "       'UserTagAvgSolvingTime', \n",
        "       #'TagAnswerRate',\n",
        "       #'CumulativeUserTagAverageAnswerRate',\n",
        "       #'CumulativeUserTagExponentialAverage', \n",
        "       'UserCumulativeTagCount',\n",
        "       'UserRecentTagAnswer', 'PreviousItemAnswer', \n",
        "       #'TestAnswerRate',\n",
        "       #'categorize_solvingTime', \n",
        "       'categorize_ItemAnswerRate',\n",
        "       'categorize_TagAnswerRate', 'categorize_TestAnswerRate',\n",
        "       'categorize_CumulativeUserItemAnswerRate',\n",
        "       #'categorize_CumulativeUserTagAverageAnswerRate',\n",
        "       'categorize_CumulativeUserTagExponentialAverage'\n",
        "    ]\n",
        "    #wandb.login()\n",
        "\n",
        "    set_seeds(seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Time\n",
        "    korea_timezone = pytz.timezone('Asia/Seoul')\n",
        "    now_korea = datetime.now(korea_timezone)\n",
        "    now_date = now_korea.strftime('%Y%m%d')\n",
        "    now_hour = now_korea.strftime('%H%M%S')\n",
        "    save_time = f\"{now_date}_{now_hour}\"\n",
        "    \n",
        "\n",
        "    ######################## DATA LOAD\n",
        "    print(\"### DATA LOAD ###\")\n",
        "    #logger.info(\"Loading data ...\")\n",
        "    train = pd.read_csv(data_dir + 'FE_v7.csv')\n",
        "\n",
        "    data = Dataset(train)\n",
        "    data, FE_train = data.split_data()\n",
        "\n",
        "    ######################## DATA PREPROCESSING\n",
        "    print(\"### DATA PREPROCESSING ###\")\n",
        "    #logger.info(\"Preparing data ...\")\n",
        "    process = Preprocess(data)\n",
        "    data = process.preprocess(cat_feats)\n",
        "\n",
        "    ######################## HYPER PARAMETER TUNING - USING OPTUNA\n",
        "    print(\"### HYPER PARAMETER TUNING - USING OPTUNA ###\")\n",
        "    print(\"number of selected features:\", len(FEATURE))\n",
        "    #wandb.init(project=\"level2-dkt\", config=vars(), entity=\"boostcamp6-recsys6\")\n",
        "    #wandb.run.name = \"yechance\" + current_time\n",
        "    #wandb.run.save()\n",
        "\n",
        "    #logger.info(\"Building Model ...\")\n",
        "    model = boosting_model(FEATURE, data)\n",
        "\n",
        "    ######################## TRAIN\n",
        "    print(\"### TRAIN ###\")\n",
        "    #logger.info(\"Start Training ...\")\n",
        "    model.training(data, FEATURE,FE_train)\n",
        "    \n",
        "    ######################## INFERENCE\n",
        "    print(\"### INFERENCE ###\")\n",
        "    #logger.info(\"\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    model.inference(data,save_time,model_type)\n",
        "\n",
        "    print( + \"_\" + save_time + \" submission file has been made\" )\n",
        "    #wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
