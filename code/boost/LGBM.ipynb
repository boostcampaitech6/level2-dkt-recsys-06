{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M_tLPVXsMpJZ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "import wandb\n",
        "from wandb.lightgbm import wandb_callback, log_summary\n",
        "from typing import TYPE_CHECKING, Callable\n",
        "from wandb.sdk.lib import telemetry as wb_telemetry\n",
        "\n",
        "from typing import List\n",
        "from lightgbm.callback import CallbackEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 중요 ###\n",
        "file_name=\"FE_v9.csv\"\n",
        "Feature = [ 'Itemseq', 'SolvingTime', 'CumulativeTime', 'UserAvgSolvingTime',\n",
        "       'RelativeUserAvgSolvingTime', 'CumulativeItemCount', 'Item_last7days',\n",
        "       'Item_last30days', 'CumulativeUserItemAcc', 'PastItemCount',\n",
        "       'UserItemElapsed', 'UserRecentItemSolvingTime', 'ItemAcc',\n",
        "       'AverageItemSolvingTime_Correct', 'AverageItemSolvingTime_Incorrect',\n",
        "       'AverageItemSolvingTime', 'RelativeItemSolvingTime',\n",
        "       'SolvingTimeClosenessDegree', 'UserTagAvgSolvingTime', 'TagAcc',\n",
        "       'CumulativeUserTagAverageAcc', 'CumulativeUserTagExponentialAverage',\n",
        "       'UserTagCount', 'UserTagElapsed',  'TestAcc', ]\n",
        "\n",
        "Categorical_Feature = ['userID', 'assessmentItemID', 'testId','KnowledgeTag',\n",
        "                       'Month','DayOfWeek', 'TimeOfDay', 'WeekOfYear', \n",
        "       'UserRecentTagAnswer',\n",
        "       'UserRecentItemAnswer',\n",
        "       'categorize_solvingTime',\n",
        "       'categorize_ItemAcc', 'categorize_TagAcc', 'categorize_TestAcc',\n",
        "       'categorize_CumulativeUserItemAcc',\n",
        "       'categorize_CumulativeUserTagAverageAcc',\n",
        "       'categorize_CumulativeUserTagExponentialAverage', 'CategorizedDegree'\n",
        "\n",
        "]\n",
        "Feature = Feature + Categorical_Feature\n",
        "\n",
        "seed = 42\n",
        "data_dir =\"../../data/\"\n",
        "output_dir =\"ouput/\"\n",
        "sweep_config_path = '/data/ephemeral/home/level2-dkt-recsys-06/code/tabular/lgbmsweepconfig.yaml'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# wandb_callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "MINIMIZE_METRICS = [\n",
        "    \"l1\",\n",
        "    \"l2\",\n",
        "    \"rmse\",\n",
        "    \"mape\",\n",
        "    \"huber\",\n",
        "    \"fair\",\n",
        "    \"poisson\",\n",
        "    \"gamma\",\n",
        "    \"binary_logloss\",\n",
        "]\n",
        "\n",
        "MAXIMIZE_METRICS = [\"map\", \"auc\", \"average_precision\"]\n",
        "\n",
        "def set_seeds(seed):\n",
        "    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def wandb_callback(log_params=True, define_metric=True) -> Callable:\n",
        "    \"\"\"Automatically integrates LightGBM with wandb.\n",
        "\n",
        "    Arguments:\n",
        "        log_params: (boolean) if True (default) logs params passed to lightgbm.train as W&B config\n",
        "        define_metric: (boolean) if True (default) capture model performance at the best step, instead of the last step, of training in your `wandb.summary`\n",
        "\n",
        "    Passing `wandb_callback` to LightGBM will:\n",
        "      - log params passed to lightgbm.train as W&B config (default).\n",
        "      - log evaluation metrics collected by LightGBM, such as rmse, accuracy etc to Weights & Biases\n",
        "      - Capture the best metric in `wandb.summary` when `define_metric=True` (default).\n",
        "\n",
        "    Use `log_summary` as an extension of this callback.\n",
        "\n",
        "    Example:\n",
        "        ```python\n",
        "        params = {\n",
        "            'boosting_type': 'gbdt',\n",
        "            'objective': 'regression',\n",
        "            .\n",
        "        }\n",
        "        gbm = lgb.train(params,\n",
        "                        lgb_train,\n",
        "                        num_boost_round=10,\n",
        "                        valid_sets=lgb_eval,\n",
        "                        valid_names=('validation'),\n",
        "                        callbacks=[wandb_callback()])\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def _define_metric(data: str, metric_name: str) -> None:\n",
        "    \n",
        "        \"\"\"Capture model performance at the best step.\n",
        "        instead of the last step, of training in your `wandb.summary`\n",
        "        \"\"\"\n",
        "        if \"loss\" in str.lower(metric_name):\n",
        "            wandb.define_metric(f\"{data}_{metric_name}\", summary=\"min\")\n",
        "        elif str.lower(metric_name) in MINIMIZE_METRICS:\n",
        "            wandb.define_metric(f\"{data}_{metric_name}\", summary=\"min\")\n",
        "        elif str.lower(metric_name) in MAXIMIZE_METRICS:\n",
        "            wandb.define_metric(f\"{data}_{metric_name}\", summary=\"max\")\n",
        "            \n",
        "    log_params_list: \"List[bool]\" = [log_params]\n",
        "    define_metric_list: \"List[bool]\" = [define_metric]\n",
        "\n",
        "    def _init(env: \"CallbackEnv\") -> None:\n",
        "        with wb_telemetry.context() as tel:\n",
        "            tel.feature.lightgbm_wandb_callback = True\n",
        "\n",
        "        wandb.config.update(env.params)\n",
        "        log_params_list[0] = False\n",
        "\n",
        "        if define_metric_list[0]:\n",
        "            for i in range(len(env.evaluation_result_list)):\n",
        "                data_type = env.evaluation_result_list[i][0]\n",
        "                metric_name = env.evaluation_result_list[i][1]\n",
        "                _define_metric(data_type, metric_name)\n",
        "\n",
        "    def _callback(env: \"CallbackEnv\") -> None:\n",
        "        if log_params_list[0]:\n",
        "            _init(env)\n",
        "        # eval_results: \"Dict[str, Dict[str, List[Any]]]\" = {}\n",
        "        # recorder = lightgbm.record_evaluation(eval_results)\n",
        "        # recorder(env)\n",
        "        eval_results = {x[0]:{x[1:][0]:x[1:][1:]} for x in env.evaluation_result_list}\n",
        "\n",
        "        for validation_key in eval_results.keys():\n",
        "            for key in eval_results[validation_key].keys():\n",
        "                 wandb.log(\n",
        "                     {validation_key + \"_\" + key: eval_results[validation_key][key][0]},\n",
        "                     commit=False,\n",
        "                 )\n",
        "        for item in eval_results:\n",
        "            if len(item) == 4:\n",
        "                wandb.log({f\"{item[0]}_{item[1]}\": item[2]}, commit=False)\n",
        "\n",
        "        # Previous log statements use commit=False. This commits them.\n",
        "        wandb.log({\"iteration\": env.iteration}, commit=True)\n",
        "\n",
        "    return _callback\n",
        "\n",
        "\n",
        "\n",
        "# 노트북의 이름 설정\n",
        "#os.environ['WANDB_NOTEBOOK_NAME'] = 'LGBM.ipynb'\n",
        "\n",
        "# YAML 파일 로드\n",
        "#with open(sweep_config_path, 'r') as file:\n",
        "    sweep_config = yaml.safe_load(file)\n",
        "\n",
        "# W&B 스위프트 설정\n",
        "#sweep_id = wandb.sweep(sweep=sweep_config, project=\"lightgbm-sweep\")\n",
        "\n",
        "# 시드 고정\n",
        "set_seeds(seed)\n",
        "\n",
        "# Time\n",
        "korea_timezone = pytz.timezone('Asia/Seoul')\n",
        "now_korea = datetime.now(korea_timezone)\n",
        "now_date = now_korea.strftime('%Y%m%d')\n",
        "now_hour = now_korea.strftime('%H%M%S')\n",
        "save_time = f\"{now_date}_{now_hour}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### DATA LOAD ###\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of selected features: 32\n",
            "### DATA PREPROCESSING ###\n",
            "2518514 train data\n",
            "7442 valid data\n"
          ]
        }
      ],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, train: pd.DataFrame,Feature,Categorical_Feature):\n",
        "        self.train = train\n",
        "        self.feature = Feature\n",
        "        self.Categorical_Feature = Categorical_Feature\n",
        "    \n",
        "\n",
        "    def restruct_data(self) -> dict:\n",
        "        # train과 test 분할\n",
        "        data = {}\n",
        "        df = self.train\n",
        "\n",
        "        # as category: integer여도 범주형으로 취급 가능\n",
        "        for feature in self.Categorical_Feature:\n",
        "            df[feature] = df[feature].astype('category')\n",
        "            \n",
        "        train = df[df[\"answerCode\"] >= 0]\n",
        "        test = df[df[\"answerCode\"] == -1]\n",
        "        data[\"train\"], data[\"test\"] = train, test\n",
        "        return data\n",
        "    \n",
        "    \n",
        "    def split_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        data의 구성\n",
        "        data['train'] : 전체 user_id에 대한 데이터(Test에 있는 User에 대해서는 이미 마지막으로 푼 문제 정보가 없음)\n",
        "        data['train_split'] : 전체 user_id별 마지막으로 푼 문제를 제외한 데이터\n",
        "        data['valid'] : 전체 user_id별 마지막으로 푼 문제에 대한 데이터\n",
        "        \"\"\"\n",
        "        data = self.restruct_data()\n",
        "        \n",
        "        train = data['train'].copy()\n",
        "        train[\"is_valid\"] = [False] * train.shape[0]\n",
        "        idx_last = train.drop_duplicates(subset=\"userID\", keep=\"last\").index\n",
        "        train.loc[idx_last, \"is_valid\"] = True\n",
        "\n",
        "        train, valid = train[train[\"is_valid\"] == False], train[train[\"is_valid\"] == True]\n",
        "        data['train'] = train.drop(\"is_valid\", axis=1)\n",
        "        data['valid'] = valid.drop(\"is_valid\", axis=1)\n",
        "\n",
        "        print(f'{data[f\"train\"].shape[0]} train data')\n",
        "        print(f'{data[f\"valid\"].shape[0]} valid data')\n",
        "\n",
        "        data[\"train_x\"] = data[\"train\"].drop(\"answerCode\", axis=1)\n",
        "        data[\"train_y\"] = data[\"train\"][\"answerCode\"]\n",
        "\n",
        "        data[\"valid_x\"] = data[\"valid\"].drop(\"answerCode\", axis=1)\n",
        "        data[\"valid_y\"] = data[\"valid\"][\"answerCode\"]\n",
        "\n",
        "        data[\"test\"] = data[\"test\"].drop(\"answerCode\", axis=1)\n",
        "\n",
        "        return data[\"train_x\"][self.feature], data[\"train_y\"], data[\"valid_x\"][self.feature], data[\"valid_y\"], data[\"test\"][self.feature]\n",
        "\n",
        "######################## DATA LOAD\n",
        "print(\"### DATA LOAD ###\")\n",
        "FE = pd.read_csv(data_dir + file_name)\n",
        "print(\"Number of selected features:\", len(Feature))\n",
        "\n",
        "######################## DATA PREPROCESSING\n",
        "print(\"### DATA PREPROCESSING ###\")\n",
        "data = Dataset(FE,Feature,Categorical_Feature)\n",
        "X_train, y_train, X_valid, y_valid, test = data.split_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.0 16.5\n",
            "True count: 2272347\n",
            "False count: 246167\n"
          ]
        }
      ],
      "source": [
        "a = X_train['AverageItemSolvingTime_Correct'].median()\n",
        "b = X_train['AverageItemSolvingTime_Incorrect'].median()\n",
        "print(a,b)\n",
        "\n",
        "X_train['CorrectGreaterThanIncorrect'] = X_train['AverageItemSolvingTime_Correct'] > X_train['AverageItemSolvingTime_Incorrect']\n",
        "\n",
        "# True와 False의 갯수 출력\n",
        "count_true = X_train['CorrectGreaterThanIncorrect'].sum()\n",
        "count_false = len(X_train) - count_true\n",
        "\n",
        "print(f\"True count: {count_true}\")\n",
        "print(f\"False count: {count_false}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Itemseq                                         int64\n",
            "SolvingTime                                   float64\n",
            "CumulativeTime                                float64\n",
            "UserAvgSolvingTime                            float64\n",
            "Difference_SolvingTime_UserAvgSolvingTime     float64\n",
            "CumulativeItemCount                             int64\n",
            "Item_last7days                                  int64\n",
            "Item_last30days                                 int64\n",
            "CumulativeUserItemAcc                         float64\n",
            "PastItemCount                                 float64\n",
            "UserItemElapsed                               float64\n",
            "ItemAcc                                       float64\n",
            "AverageItemSolvingTime_Correct                float64\n",
            "AverageItemSolvingTime_Incorrect              float64\n",
            "AverageItemSolvingTime                        float64\n",
            "Difference_SolvingTime_AvgItemSolvingTime     float64\n",
            "UserTagAvgSolvingTime                         float64\n",
            "TagAcc                                        float64\n",
            "CumulativeUserTagAverageAcc                   float64\n",
            "CumulativeUserTagExponentialAverage           float64\n",
            "UserTagCount                                    int64\n",
            "UserTagElapsed                                float64\n",
            "PastTagSolvingTime                            float64\n",
            "TestAcc                                       float64\n",
            "userID                                       category\n",
            "assessmentItemID                             category\n",
            "testId                                       category\n",
            "KnowledgeTag                                 category\n",
            "Month                                        category\n",
            "DayOfWeek                                    category\n",
            "TimeOfDay                                    category\n",
            "WeekOfYear                                   category\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(X_train.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### TRAIN ###\n"
          ]
        }
      ],
      "source": [
        "def train():\n",
        "    auc = 0\n",
        "    acc = 0\n",
        "    test_preds = np.zeros(len(test))\n",
        "    \n",
        "    default_config = {\n",
        "    \"num_leaves\": 10,  # 최소값 10\n",
        "    \"learning_rate\": 0.0001,  # 최소값 0.0001\n",
        "    \"max_depth\": -1,  # -1 (깊이 제한 없음)\n",
        "    \"min_data_in_leaf\": 20,  # 최소값 20\n",
        "    \"feature_fraction\": 0.6,  # 최소값 0.6\n",
        "    \"bagging_fraction\": 0.6,  # 최소값 0.6\n",
        "    \"bagging_freq\": 0,  # 최소값 0\n",
        "    \"lambda_l1\": 0.0,  # 최소값 0.0\n",
        "    \"lambda_l2\": 0.0,  # 최소값 0.0\n",
        "    \"cat_smooth\": 10,  # 최소값 10 \n",
        "    }   \n",
        "\n",
        "    current_params = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"metric\": [\"auc\"],\n",
        "        \"device\": \"cpu\",\n",
        "       # \"num_leaves\": wandb.config.num_leaves,\n",
        "       # \"learning_rate\": wandb.config.learning_rate,\n",
        "       # \"max_depth\": wandb.config.max_depth,\n",
        "       # \"min_data_in_leaf\": wandb.config.min_data_in_leaf,\n",
        "       # \"feature_fraction\": wandb.config.feature_fraction,\n",
        "       # \"bagging_fraction\": wandb.config.bagging_fraction,\n",
        "       # \"bagging_freq\": wandb.config.bagging_freq,\n",
        "       # \"lambda_l1\": wandb.config.lambda_l1,\n",
        "       # \"lambda_l2\": wandb.config.lambda_l2,\n",
        "       # \"cat_smooth\": wandb.config.cat_smooth,\n",
        "    }\n",
        "\n",
        "    #wandb.init(config=default_config)\n",
        "    #wandb.run.name = f\"{save_time} yechan\"\n",
        "\n",
        "    \n",
        "    \n",
        "    lgb_train = lgb.Dataset(X_train, label = y_train)\n",
        "    lgb_valid = lgb.Dataset(X_valid, label = y_valid)\n",
        "\n",
        "    model = lgb.train(\n",
        "        current_params,\n",
        "        lgb_train,\n",
        "        valid_sets=[lgb_train,lgb_valid],\n",
        "        num_boost_round=500,\n",
        "        #callbacks=[wandb_callback(log_params=True, define_metric=True),lgb.early_stopping(30)],\n",
        "        categorical_feature= Categorical_Feature,\n",
        "    )\n",
        "    \n",
        "    \n",
        "    # Prediction\n",
        "    y_pred_proba = model.predict(X_valid)\n",
        "    \n",
        "    # Calculate Accuracy and AUC\n",
        "    acc = accuracy_score(y_valid, np.where(y_pred_proba >= 0.5, 1, 0))\n",
        "    auc = roc_auc_score(y_valid, y_pred_proba)\n",
        "    print(f\"VALID AUC : {auc} ACC : {acc}\\n\")\n",
        "\n",
        "    test_preds += model.predict(test)\n",
        "    #wandb.log({\"auc\": auc, \"accuracy\": acc})\n",
        "    #wandb.finish()\n",
        "            \n",
        "    write_path = os.path.join(\n",
        "        output_dir,\n",
        "        f\"auc:{auc} acc:{acc} LGBM_{save_time}.csv\",\n",
        "    )\n",
        "    \n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)  \n",
        "\n",
        "    with open(write_path, \"w\", encoding=\"utf8\") as w:\n",
        "        print(\"writing prediction : {}\".format(write_path))\n",
        "        w.write(\"id,prediction\\n\")\n",
        "        for id, p in enumerate(test_preds):\n",
        "            w.write(\"{},{}\\n\".format(id, p))\n",
        "\n",
        "    importance = model.feature_importances_\n",
        "    \n",
        "    # DataFrame 생성하고 정렬\n",
        "    fi_df = pd.DataFrame({'Feature Names': np.array(Feature), 'Feature Importance': np.array(importance)})\n",
        "    fi_df = fi_df.sort_values(by='Feature Importance', ascending=False)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"LGBM Feature Importance:\")\n",
        "    print(fi_df)\n",
        "\n",
        "    print( \"LGBM\" + \"_\" + save_time + \" Submission file has been made\")\n",
        "\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######################## TRAIN\n",
        "print(\"### TRAIN ###\")\n",
        "#wandb.agent(sweep_id, train)\n",
        "train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "lightgcn",
      "language": "python",
      "name": "lightgcn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
